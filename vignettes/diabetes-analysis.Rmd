---
title: "Multi-Path AIC Selection: Diabetes Progression Analysis"
author: "Michael Obuobi, Jinchen Jiang, Far Rahmati"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multi-Path AIC Selection: Diabetes Progression Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

# Introduction

This vignette demonstrates the multi-path AIC selection procedure on the diabetes progression dataset from Efron et al. (2004). The dataset contains 442 patients with 10 baseline predictors measuring various health indicators, and the goal is to predict a continuous measure of disease progression one year after baseline.

We extend the predictor set by including second-order interaction and quadratic terms, resulting in approximately 60 predictors. This creates a challenging variable selection problem ideal for demonstrating the multi-path approach.

# Load the Package and Data
```{r load-packages, message=FALSE}
library(multipathaic)
library(lars)
library(caret)
```

```{r load-data}
# Load diabetes data
data(diabetes)

head(diabetes)

# Extract response and predictors
y <- diabetes$y
X <- as.data.frame(diabetes$x)

# Display original dimensions
cat("Original dimensions:", nrow(X), "observations,", ncol(X), "predictors\n")
```

# Feature Engineering

We create second-order polynomial features (interactions and quadratics) to increase the predictor space:
```{r feature-engineering}
# Create formula for second-order terms
formula_expanded <- as.formula(
  paste("~ (", paste(colnames(X), collapse = " + "), ")^2 + I(", 
        paste(colnames(X), "^2", collapse = ") + I("), ")")
)

# Generate expanded feature matrix
X_expanded <- as.data.frame(model.matrix(formula_expanded, data = X))
X_expanded <- X_expanded[, -1]  # remove intercept

# Clean column names to avoid formula issues
colnames(X_expanded) <- make.names(colnames(X_expanded), unique = TRUE)

cat("Expanded dimensions:", ncol(X_expanded), "predictors\n")

# Standardize predictors
X_scaled <- as.data.frame(scale(X_expanded))
```

# Train/Test Split

We use a 70/30 train/test split to evaluate model performance:
```{r train-test-split}
set.seed(123)
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)

X_train <- X_scaled[train_idx, ]
X_test <- X_scaled[-train_idx, ]
y_train <- y[train_idx]
y_test <- y[-train_idx]

cat("Training set:", nrow(X_train), "samples\n")
cat("Test set:", nrow(X_test), "samples\n")
```

# Multi-Path AIC Procedure

We now apply the complete multi-path AIC selection pipeline:

## Algorithm 1: Multi-Path Forward Selection

We explore multiple competitive model paths using AIC-based branching:
```{r build-paths}
forest <- build_paths(
  X = X_train,
  y = y_train,
  family = "gaussian",
  K = 10,           # Maximum 10 steps
  eps = 1e-6,       # Minimum AIC improvement
  delta = 2,        # AIC tolerance for branching
  L = 50,           # Keep top 50 models per step
  verbose = FALSE
)

print(forest)

cat("Total models explored:", nrow(forest$all_models), "\n")
cat("Models at final step:", nrow(forest$path_forest$frontiers[[length(forest$path_forest$frontiers)]]), "\n")
```

## Algorithm 2: Stability Estimation

We assess variable stability across 50 bootstrap resamples:
```{r stability}
stab <- stability(
  X = X_train,
  y = y_train,
  family = "gaussian",
  K = 10,
  eps = 1e-6,
  delta = 2,
  L = 50,
  B = 50,           # 50 bootstrap resamples
  resample_fraction = 0.8,
  verbose = FALSE
)

print(stab)

# Display top stable variables
cat("\nTop 10 most stable variables:\n")
print(round(head(stab$pi, 10), 3))
```

## Algorithm 3: Plausible Model Selection

We filter models using AIC tolerance ($\Delta$ = 2) and stability threshold ($\tau$ = 0.6):

**Parameter Justification:**

- **Delta = 2**: Following the standard guideline (Burnham & Anderson, 2002) that models within 2 AIC units have essentially equivalent empirical support.
- **tua = 0.6**: Ensures we retain only models built from variables appearing in >60% of resamples, providing confidence that selected variables are robust to sampling variation.
```{r plausible-models}
plaus <- plausible_models(
  forest = forest,
  pi = stab$pi,
  Delta = 2,
  tau = 0.6,
  verbose = FALSE
)

print(plaus)

cat("Number of plausible models:", nrow(plaus$plausible_models), "\n")
cat("Best AIC:", round(plaus$best_aic, 2), "\n\n")

# Display variable summary
cat("Variable inclusion summary:\n")
print(head(plaus$summary, 15))
```

# Model Evaluation on Test Set

We evaluate the best plausible model on the held-out test set:
```{r test-evaluation}
if (nrow(plaus$plausible_models) > 0) {
  # Extract variables from best model
  best_model_vars <- plaus$plausible_models$model[[1]]
  
  # Fit on training data
  train_df <- data.frame(y = y_train, X_train[, best_model_vars, drop = FALSE])
  final_fit <- lm(y ~ ., data = train_df)
  
  # Predict on test set
  test_df <- X_test[, best_model_vars, drop = FALSE]
  y_pred <- predict(final_fit, newdata = test_df)
  
  # Compute metrics
  test_rmse <- sqrt(mean((y_test - y_pred)^2))
  train_pred <- predict(final_fit)
  train_rmse <- sqrt(mean((y_train - train_pred)^2))
  test_cor <- cor(y_test, y_pred)
  
  cat("\n=== Best Model Performance ===\n")
  cat("Variables selected:", length(best_model_vars), "\n")
  cat("Variable names:", paste(best_model_vars, collapse = ", "), "\n\n")
  cat("Training RMSE:", round(train_rmse, 3), "\n")
  cat("Test RMSE:", round(test_rmse, 3), "\n")
  cat("Test Correlation:", round(test_cor, 3), "\n")
  cat("R-squared (train):", round(summary(final_fit)$r.squared, 3), "\n")
}
```

# Summary and Parameter Justification

This analysis demonstrates the multi-path AIC selection framework on a challenging regression problem with many correlated predictors. The procedure successfully:

1. **Explored multiple competitive model paths** rather than committing to a single greedy sequence
2. **Identified stable variables** that consistently appear across bootstrap resamples
3. **Selected plausible models** that balance goodness-of-fit (low AIC) with stability (reliable variables)
4. **Achieved good predictive performance** on held-out test data

The final model includes `r if(nrow(plaus$plausible_models) > 0) length(plaus$plausible_models$model[[1]]) else "NA"` predictors and demonstrates that the multi-path approach can effectively navigate high-dimensional predictor spaces while maintaining model parsimony.



## Recommended Parameter Choices

The following parameters were used throughout this analysis:

- **K = min(p, 10)**: Maximum number of forward selection steps, limited to avoid overfitting
- **eps = 1e-6**: Minimum AIC improvement threshold (very small to allow exploration)
- **delta = 2**: AIC tolerance for keeping near-tie models (follows standard guideline that models within 2 AIC units are indistinguishable)
- **L = 25-100**: Maximum models kept per level to control computational growth
- **B = 50-100**: Number of bootstrap resamples for stability (balance between precision and computation)
- **Delta = 2**: AIC tolerance for plausibility filter (same rationale as delta)
- **tau = 0.6**: Minimum average stability threshold (keeps models with variables appearing in >60% of resamples, indicating strong stability)

## Justification

**Delta = 2** is justified by the widely-accepted statistical principle that models within 2 AIC units have essentially equivalent support from the data (Burnham & Anderson, 2002).

**tau = 0.6** ensures we retain only models built from variables that consistently appear across the majority of resamples, providing confidence that the selected variables are robust to sampling variation rather than artifacts of a particular dataset.

These choices balance model exploration (allowing multiple competitive paths) with stability (filtering for reliable predictors).



# References

Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. *The Annals of Statistics*, 32(2), 407-499.

Burnham, K. P., & Anderson, D. R. (2002). *Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach* (2nd ed.). Springer.

